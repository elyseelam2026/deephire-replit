For the Staging stage

AI verification ‚Üí moving verified candidates to production

I want to be more detailed in the verification processes, can you give me some suggestion? 
ChatGPT said:

Excellent question, Elysee ‚Äî this is where your system can truly stand out.

What you‚Äôre describing ‚Äî the AI verification layer that filters raw scraped data before it reaches your production database ‚Äî is the most critical ‚Äútrust‚Äù layer in your AI recruiter pipeline. It determines your overall data quality, credibility with clients, and defensibility as a business.

Let‚Äôs go deep into how to design it properly.

üß© Purpose of the Verification Layer

The goal of this layer is to:

Clean, standardize, and validate scraped data.

Confirm that each candidate record is real, consistent, and current.

Enrich the data with missing or inferred details.

Flag or quarantine suspicious or low-confidence data before it contaminates the production database.

üèóÔ∏è Recommended Structure for the AI Verification Stage

Below is a detailed breakdown of the sub-processes you can implement.

[RAW CANDIDATE DATA]
     ‚îÇ
     ‚ñº
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üß†  AI VERIFICATION STAGE
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1. Data Integrity Check
  2. Entity Normalization
  3. Cross-Source Validation
  4. Title & Role Classification
  5. Company Verification
  6. Skill Extraction & Validation
  7. Time & Career Consistency
  8. AI Enrichment
  9. Confidence Scoring
 10. Human Review (optional)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚îÇ
     ‚ñº
[VERIFIED CANDIDATES ‚Üí PRODUCTION DB]

üß© 1. Data Integrity Check

Purpose: Make sure the data is complete, machine-readable, and free of obvious errors.

Examples:

Remove records missing name, company, or title.

Check for encoding issues (e.g., broken characters).

Eliminate duplicates using fuzzy name + company matching (e.g., ‚ÄúJohn A. Smith‚Äù ‚âà ‚ÄúJ. Smith‚Äù).

üß© 2. Entity Normalization

Purpose: Standardize names, titles, and companies to ensure consistent comparison and searching.

Examples:

Convert ‚ÄúIntl. Business Machines‚Äù ‚Üí ‚ÄúIBM‚Äù.

Standardize titles: ‚ÄúSr. Mgr HR‚Äù ‚Üí ‚ÄúSenior Manager, Human Resources‚Äù.

Remove noise from job titles (‚Äú(contract)‚Äù, ‚Äú(remote)‚Äù).

üîß Tools: Regex rules + AI-powered name/company/title normalizer using OpenAI or HuggingFace models.

üß© 3. Cross-Source Validation

Purpose: Confirm data reliability by comparing with multiple sources.

Examples:

If candidate appears on company site + LinkedIn ‚Üí high confidence.

If role duration differs between sources ‚Üí flag for human review.

If profile photo / job title mismatch ‚Üí flag as potential outdated data.

‚úÖ You can assign a confidence score per data point (name, title, company, duration).

üß© 4. Title & Role Classification

Purpose: Translate unstructured titles into structured, comparable categories.

Examples:

‚ÄúTalent Partner‚Äù, ‚ÄúRecruitment Lead‚Äù ‚Üí classified under ‚ÄúTalent Acquisition‚Äù.

‚ÄúVP, Data & Insights‚Äù ‚Üí under ‚ÄúData Science / Analytics‚Äù.

üîß Tools: A fine-tuned model (e.g., BERT or OpenAI fine-tune) to map messy titles into standard taxonomy.

üß© 5. Company Verification

Purpose: Confirm that the company exists, and the candidate‚Äôs role makes sense there.

Examples:

Check against a company database (Crunchbase, OpenCorporates, or your internal company index).

AI checks whether the company‚Äôs industry matches the candidate‚Äôs job function.
‚Üí ‚ÄúPortfolio Manager‚Äù at ‚ÄúTemasek‚Äù ‚úÖ plausible.
‚Üí ‚ÄúPortfolio Manager‚Äù at ‚ÄúKFC‚Äù ‚ùå implausible (flag).

üß© 6. Skill Extraction & Validation

Purpose: Extract and verify skills implied by job titles, descriptions, or company context.

Examples:

Extract explicit or inferred skills (‚ÄúData Scientist‚Äù ‚Üí ‚ÄúPython, Machine Learning, Statistics‚Äù).

Cross-check consistency:

‚ÄúData Scientist‚Äù with ‚ÄúPhotoshop, Illustrator‚Äù ‚Üí flag mismatch.

Optionally enrich with inferred skills from company context (e.g., ‚ÄúGoogle Ads Manager‚Äù ‚Üí ‚ÄúSEM, PPC‚Äù).

üß© 7. Time & Career Consistency Check

Purpose: Ensure the career timeline makes sense.

Examples:

Check for overlapping job dates or missing transitions.

Flag sudden role regressions (e.g., ‚ÄúVP‚Äù ‚Üí ‚ÄúAnalyst‚Äù).

Detect improbable jumps (e.g., ‚ÄúIntern‚Äù ‚Üí ‚ÄúDirector‚Äù in 6 months).

‚úÖ AI can score career stability & growth trajectory.

üß© 8. AI Enrichment

Purpose: Fill in missing but inferable details.

Examples:

Infer seniority level (‚ÄúHead of Marketing‚Äù ‚Üí Senior Management).

Predict industry if not stated (based on company + role).

Generate summary sentences:

‚Äú10+ years in HR leadership across regional MNCs.‚Äù

üß© 9. Confidence Scoring

Purpose: Quantify how trustworthy the candidate record is.

Examples of dimensions:

Source reliability (company website > LinkedIn > scraped 3rd party).

Data completeness (100% filled = higher confidence).

Internal consistency (no contradictions = higher confidence).

Role plausibility (title vs company vs skills).

‚úÖ Combine all into an overall verification confidence score (0‚Äì100).

Candidates with:

80 ‚Üí auto-promote to production DB.

50‚Äì80 ‚Üí send to ‚ÄúAI review / enrichment‚Äù queue.

<50 ‚Üí quarantine or discard.

üß© 10. Human Review (Optional)

For borderline or high-value candidates.

Reviewer can override AI confidence, correct inconsistencies, or confirm data before final promotion.

Each human decision becomes training data to improve AI verification accuracy.

‚öôÔ∏è Example of a Verification Record Schema
Field	Description	Example	Confidence
name	candidate name	John A. Smith	0.98
company	current employer	Temasek Holdings	0.95
title	current title	VP, Data Science	0.92
start_date	role start date	2021-07	0.85
skills	extracted list	[Python, ML, Portfolio Analytics]	0.90
verified_sources	cross-validated URLs	[LinkedIn, company site]	‚Äî
ai_summary	generated profile summary	‚ÄúData leader with 12 years in investment analytics‚Äù	‚Äî
overall_confidence	final score	0.93	
üß† Bonus Ideas

Automated discrepancy reporting: Store reasons for low confidence (e.g., ‚ÄúCompany not found‚Äù, ‚ÄúDate overlap detected‚Äù).

Feedback API: When a human corrects data, AI learns the correction pattern.

Temporal re-verification: Set a ‚Äústaleness threshold‚Äù (e.g., re-verify every 6‚Äì12 months).

Explainable audit trail: Log every step of verification for compliance and transparency.

If you‚Äôd like, I can help you design a Replit-compatible verification pipeline (Python code structure) with placeholders for each verification sub-step ‚Äî so you can start implementing it modularly without breaking your current system.

Would you like me to draft that for you next?

You said:
Yes please.   Also, I want to create the career history structure for each candidates so that by them time the AI collects the biography, they can dissemble into each career history or the other way round, if they have the career history like what displayed in Linkedin.  They can generate a passage biography 
ChatGPT said:

Beautiful ‚Äî now you‚Äôre truly thinking like a system architect for an AI recruiter, Elysee.

You‚Äôre not just collecting names and titles ‚Äî you‚Äôre thinking about biographical reasoning, where the AI can move fluidly between:

Structured data (career history objects) ‚ü∑

Unstructured narratives (biographies, summaries).

That‚Äôs exactly how a seasoned recruiter interprets profiles: they see both the structured career journey and the storytelling behind it.

Let‚Äôs design your full Verification + Career History Structure so your Replit project becomes the foundation of this capability.

üß© PART 1: Verification Pipeline ‚Äî Modular Structure

Here‚Äôs a Replit-compatible Python scaffold you can start from.
Each function corresponds to one of the verification steps we discussed.
You can fill in AI logic, regex, or external API calls later.

# verification_pipeline.py
from datetime import datetime
from typing import List, Dict, Any
import re

# ===== 1Ô∏è‚É£ Data Integrity Check =====
def check_data_integrity(candidate: Dict[str, Any]) -> bool:
    required_fields = ["name", "company", "title"]
    return all(candidate.get(f) for f in required_fields)

# ===== 2Ô∏è‚É£ Entity Normalization =====
def normalize_entities(candidate: Dict[str, Any]) -> Dict[str, Any]:
    candidate["name"] = candidate["name"].title().strip()
    candidate["company"] = re.sub(r'\bIntl\.?\b', 'International', candidate["company"], flags=re.I)
    candidate["title"] = candidate["title"].replace("Sr.", "Senior").strip()
    return candidate

# ===== 3Ô∏è‚É£ Cross-Source Validation =====
def cross_source_validation(candidate: Dict[str, Any], sources: List[Dict[str, Any]]) -> float:
    """Return confidence based on consistency across sources."""
    matches = sum(1 for src in sources if src.get("title") == candidate["title"])
    return min(1.0, matches / max(1, len(sources)))

# ===== 4Ô∏è‚É£ Title & Role Classification =====
def classify_role(title: str) -> str:
    title_lower = title.lower()
    if "data" in title_lower: return "Data Science"
    if "hr" in title_lower or "talent" in title_lower: return "Human Resources"
    if "finance" in title_lower or "cfo" in title_lower: return "Finance"
    return "General Management"

# ===== 5Ô∏è‚É£ Company Verification =====
def verify_company(candidate: Dict[str, Any], company_list: List[str]) -> bool:
    return candidate["company"] in company_list

# ===== 6Ô∏è‚É£ Skill Extraction =====
def extract_skills(title: str, description: str = "") -> List[str]:
    mapping = {
        "data": ["Python", "Machine Learning", "Statistics"],
        "hr": ["Recruitment", "Learning", "Employee Relations"],
        "finance": ["Budgeting", "Financial Analysis"]
    }
    key = "data" if "data" in title.lower() else \
          "hr" if "hr" in title.lower() or "talent" in title.lower() else \
          "finance" if "finance" in title.lower() else None
    return mapping.get(key, [])

# ===== 7Ô∏è‚É£ Career Timeline Consistency =====
def check_career_timeline(history: List[Dict[str, Any]]) -> float:
    """Check for overlapping or implausible job sequences."""
    history_sorted = sorted(history, key=lambda x: x.get("start_date", ""))
    consistency = 1.0
    for i in range(1, len(history_sorted)):
        prev_end = history_sorted[i-1].get("end_date")
        start = history_sorted[i].get("start_date")
        if prev_end and start and prev_end > start:
            consistency -= 0.2
    return max(consistency, 0.0)

# ===== 8Ô∏è‚É£ Confidence Score Aggregation =====
def compute_confidence(candidate: Dict[str, Any]) -> float:
    weights = {
        "data_integrity": 0.2,
        "cross_source": 0.3,
        "timeline": 0.3,
        "company_verified": 0.2
    }
    score = (
        weights["data_integrity"] * candidate["meta"]["data_integrity"] +
        weights["cross_source"] * candidate["meta"]["cross_source"] +
        weights["timeline"] * candidate["meta"]["timeline"] +
        weights["company_verified"] * (1.0 if candidate["meta"]["company_verified"] else 0)
    )
    return round(score, 2)

# ===== 9Ô∏è‚É£ Verification Orchestrator =====
def verify_candidate(candidate: Dict[str, Any], sources: List[Dict[str, Any]], company_list: List[str]) -> Dict[str, Any]:
    candidate["meta"] = {}
    candidate["meta"]["data_integrity"] = 1.0 if check_data_integrity(candidate) else 0.0
    candidate = normalize_entities(candidate)
    candidate["meta"]["cross_source"] = cross_source_validation(candidate, sources)
    candidate["role_class"] = classify_role(candidate["title"])
    candidate["meta"]["company_verified"] = verify_company(candidate, company_list)
    candidate["skills"] = extract_skills(candidate["title"])
    candidate["meta"]["timeline"] = check_career_timeline(candidate.get("career_history", []))
    candidate["confidence"] = compute_confidence(candidate)
    return candidate


üü¢ This modular structure allows you to expand each verification step later
‚Äî by calling APIs, adding GPT reasoning, or linking with vector search.

üß© PART 2: Career History Data Structure

Each candidate‚Äôs career history should be a list of structured roles, each with consistent keys.
This makes it easy for AI to:

Parse existing structured data into biography text.

Reverse-engineer biographies into structured timelines.

Suggested Schema:

career_history = [
    {
        "company": "Temasek Holdings",
        "title": "Vice President, Data Science",
        "start_date": "2018-09",
        "end_date": "2022-09",
        "description": "Led investment analytics projects using machine learning and data-driven portfolio analysis.",
        "location": "Singapore",
        "achievements": [
            "Developed predictive models for equity investments.",
            "Built NLP tools for financial data analysis."
        ]
    },
    {
        "company": "GIC",
        "title": "Assistant Vice President, Data & Analytics",
        "start_date": "2015-10",
        "end_date": "2018-09",
        "description": "Collaborated with investment teams to leverage data science for portfolio optimization.",
        "location": "Singapore",
        "achievements": [
            "Implemented analytics dashboards for investment insights.",
            "Reduced data processing time by 30%."
        ]
    }
]

üß© PART 3: Biographical Generation

Once the AI has structured data like above, you can generate a natural-language biography automatically:

def generate_biography(candidate: Dict[str, Any]) -> str:
    bio = f"{candidate['name']} is a professional with extensive experience in {candidate['role_class'].lower()}."
    for role in candidate.get("career_history", []):
        years = f" from {role['start_date']} to {role['end_date']}" if role.get("end_date") else " since " + role['start_date']
        bio += f" Previously, {candidate['name']} worked at {role['company']} as {role['title']}{years}, where they {role['description'].lower()}"
        if role.get("achievements"):
            bio += " Key achievements include: " + ", ".join(role['achievements']) + "."
    bio += f" Overall, {candidate['name']} brings a strong background in {', '.join(candidate.get('skills', []))}."
    return bio


‚úÖ Bidirectional Capability:

If you have structured data, this generates a smooth biography.

If you only have a biography text, you can later train AI to parse and reconstruct structured career history (reverse direction).

üß† Long-Term Vision

Once your verification & career history structure are stable:

You can store both structured (JSON) and unstructured (text) representations for every candidate.

Each time the AI learns, it improves at moving between the two forms.

This duality is how your AI will one day ‚Äúread and write like a recruiter.‚Äù

Would you like me to show you next how to build the reverse pipeline ‚Äî
i.e., when you only have a biography text, the AI automatically breaks it down into a structured career_history list (for database storage)?

You said:
And we need to have a QA to make sure that no matter is career structure or biography we need to have proofread by AI or/and human before approving the record.  
ChatGPT said:

Perfect ‚Äî this is a critical layer in making your system trustworthy and recruiter-grade.

You‚Äôre now thinking in terms of data governance, not just data processing ‚Äî which is exactly what separates a real ‚ÄúAI recruiter platform‚Äù from a r√©sum√© parser.

Let‚Äôs design this QA (Quality Assurance) layer in detail, so it can work for both structured career data and biographical text.

üß© QA LAYER ‚Äî Purpose & Workflow
üéØ Objective

Ensure that every candidate record ‚Äî whether in structured form (career_history) or narrative form (biography) ‚Äî meets professional-level accuracy, consistency, and tone before moving into production.

üîÅ Verification Flow (extended)
Data Source ‚Üí AI Parsing ‚Üí Staging Database
         ‚Üì
     Verification Pipeline (we built)
         ‚Üì
     QA Review (AI + Human)
         ‚Üì
     Production Database

üß± STRUCTURE OF THE QA MODULE

Here‚Äôs how you can integrate a QA stage into your Replit system (or any Python backend).

# qa_module.py
from typing import Dict, Any, List
import re

# ===== 1Ô∏è‚É£ Automated QA for Career Structure =====
def qa_structured_data(candidate: Dict[str, Any]) -> Dict[str, Any]:
    issues = []

    # Check date validity
    for role in candidate.get("career_history", []):
        if not re.match(r"\d{4}-\d{2}", role.get("start_date", "")):
            issues.append(f"Invalid start date format for {role['company']}")
        if role.get("end_date") and not re.match(r"\d{4}-\d{2}", role["end_date"]):
            issues.append(f"Invalid end date format for {role['company']}")
        if role.get("end_date") and role["start_date"] > role["end_date"]:
            issues.append(f"End date earlier than start date in {role['company']}")

    # Check for missing key fields
    for key in ["company", "title", "start_date"]:
        if any(not role.get(key) for role in candidate["career_history"]):
            issues.append(f"Missing '{key}' in one or more career entries")

    # Check for duplicate roles
    titles = [r["title"].lower() for r in candidate["career_history"]]
    if len(set(titles)) != len(titles):
        issues.append("Duplicate role titles detected")

    candidate["qa"] = {"issues": issues, "status": "pass" if not issues else "needs_review"}
    return candidate

# ===== 2Ô∏è‚É£ Automated QA for Biographical Text =====
def qa_biography(bio: str) -> Dict[str, Any]:
    issues = []
    # Basic grammar/sanity checks
    if len(bio.split()) < 50:
        issues.append("Biography too short, may lack sufficient detail.")
    if not bio[0].isupper():
        issues.append("Biography should start with a capital letter.")
    if not bio.strip().endswith("."):
        issues.append("Biography should end with a period.")

    # Red flag check: placeholders, typos, profanity
    if "lorem" in bio.lower():
        issues.append("Placeholder text detected.")
    if re.search(r"\b(fuck|shit|damn)\b", bio.lower()):
        issues.append("Inappropriate language detected.")

    return {"issues": issues, "status": "pass" if not issues else "needs_review"}

# ===== 3Ô∏è‚É£ Combined QA Orchestrator =====
def qa_verification(candidate: Dict[str, Any]) -> Dict[str, Any]:
    """Run QA checks on both structured and unstructured fields."""
    structured_result = qa_structured_data(candidate)
    bio_result = qa_biography(candidate.get("biography", ""))

    candidate["qa"]["bio_issues"] = bio_result["issues"]
    candidate["qa"]["final_status"] = (
        "pass" if structured_result["qa"]["status"] == "pass" and bio_result["status"] == "pass"
        else "needs_review"
    )
    return candidate

üß† HUMAN-IN-THE-LOOP QA

Once the automated QA flags issues, the system should queue those records for human review.

You can store them in a table (e.g., qa_pending) with fields like:

Candidate ID	QA Status	Issues Summary	Reviewer	Notes	Last Reviewed
101	needs_review	Missing end date, short biography	Elysee	OK after edit	2025-10-09

Later you can give this table a web UI ‚Äî even something simple like a Streamlit or Flask dashboard ‚Äî for reviewers to approve or reject entries.

ü§ñ AI-ASSISTED QA (Optional Next Step)

You can layer AI proofreading for biographies using GPT or similar API:

def ai_proofread_biography(bio: str) -> str:
    """Call GPT to suggest improved grammar, tone, and structure."""
    prompt = f"""
    You are a professional copy editor for executive recruiters.
    Proofread and refine this biography for clarity and tone.
    Do not change factual content:
    {bio}
    """
    # response = openai.ChatCompletion.create(...)
    # return response["choices"][0]["message"]["content"]
    return bio  # placeholder


You can then have:

AI QA ‚Üí Human Approval, or

Human QA only for AI-flagged cases (to scale your review process).

‚úÖ FINAL DECISION LOGIC

Once the QA is done:

if candidate["qa"]["final_status"] == "pass":
    move_to_production(candidate)
else:
    flag_for_human_review(candidate)

üîç Optional: QA Dashboard (Later Stage)

You can easily visualize this with:

‚ÄúPending review‚Äù queue

‚ÄúPassed / Failed‚Äù filters

‚ÄúAI suggestions‚Äù side-by-side view
This helps you track which records are verified and which need intervention.

Would you like me to show you next how to design the QA dashboard interface (so your team can review AI-flagged records inside Replit or a small web app)?
It can be built lightweight using Streamlit or FastAPI + HTML templates.

You said:
yes please
ChatGPT said:

Excellent ‚Äî this is where your system starts feeling alive and operational, Elysee.
We‚Äôll design a QA Dashboard Interface that your (or your team‚Äôs) human reviewers can use to verify candidate profiles that have been flagged by the AI.

Here‚Äôs a breakdown of the plan and then a Replit-ready prototype.

üß© Overview ‚Äî What the QA Dashboard Does
üéØ Purpose

Let your QA reviewer (human) see:

Candidate profile (career history + biography)

AI-detected issues

Option to approve, edit, or reject

System updates the QA status and moves the record appropriately.

‚öôÔ∏è Tech Choice

We‚Äôll use Streamlit because it runs perfectly in Replit, requires no front-end coding, and allows instant UI prototyping.

üß± System Architecture
Candidate Data (Staging DB)
      ‚Üì
AI Verification ‚Üí QA Flagging
      ‚Üì
QA Dashboard (Streamlit)
      ‚Üì
Approve / Reject ‚Üí Update QA Table
      ‚Üì
Production Database

üßë‚Äçüíª Step-by-Step Implementation in Replit
1Ô∏è‚É£ Install Dependencies

In Replit shell:

pip install streamlit pandas

2Ô∏è‚É£ Create a file named qa_dashboard.py

Paste this code:

import streamlit as st
import pandas as pd
import json

# ====== Mock Data ======
# In practice, you'll pull this from your staging database
with open("qa_pending.json", "r") as f:
    candidates = json.load(f)

# Convert to DataFrame for easier viewing
df = pd.DataFrame(candidates)

# ====== Sidebar Filters ======
st.sidebar.title("QA Dashboard Filters")
qa_filter = st.sidebar.selectbox("Show candidates:", ["All", "Needs Review", "Passed"])

if qa_filter == "Needs Review":
    df = df[df["qa_status"] == "needs_review"]
elif qa_filter == "Passed":
    df = df[df["qa_status"] == "pass"]

# ====== Candidate Viewer ======
st.title("AI Recruiter ‚Äì QA Review Dashboard")

if not len(df):
    st.write("‚úÖ No candidates currently pending QA review.")
else:
    for idx, row in df.iterrows():
        st.markdown("---")
        st.subheader(f"Candidate: {row['name']}")
        st.write(f"**Biography:** {row['biography']}")
        st.json(row["career_history"])
        st.write(f"**AI Issues:** {row['qa_issues']}")

        with st.expander("AI Proofread Suggestion"):
            st.write(row.get("ai_proofread", "No suggestion yet."))

        action = st.radio(
            f"QA Action for {row['name']}:",
            ["Pending", "Approve", "Reject", "Edit & Approve"],
            key=f"action_{idx}",
            horizontal=True,
        )

        if action == "Approve":
            row["qa_status"] = "pass"
            st.success(f"{row['name']} approved and moved to production.")
        elif action == "Reject":
            row["qa_status"] = "rejected"
            st.error(f"{row['name']} rejected for recheck.")
        elif action == "Edit & Approve":
            edited_bio = st.text_area("Edit Biography", row["biography"], key=f"editbio_{idx}")
            row["biography"] = edited_bio
            row["qa_status"] = "pass"
            st.success(f"{row['name']} updated and approved.")

# ====== Save updates back ======
if st.button("üíæ Save QA Updates"):
    df.to_json("qa_pending.json", orient="records", indent=2)
    st.success("QA records updated successfully!")

3Ô∏è‚É£ Create a sample file named qa_pending.json
[
  {
    "name": "Alex Chan",
    "biography": "Alex has over 15 years of experience in finance and HR leadership roles.",
    "career_history": [
      {"company": "HSBC", "title": "HR Director", "start_date": "2015-06", "end_date": "2023-08"},
      {"company": "PwC", "title": "HR Manager", "start_date": "2010-01", "end_date": "2015-05"}
    ],
    "qa_status": "needs_review",
    "qa_issues": ["Biography too short", "Missing description of achievements"],
    "ai_proofread": "Alex has over 15 years of progressive experience in human resources and finance leadership, focusing on organizational development and talent strategy."
  },
  {
    "name": "Jamie Li",
    "biography": "Jamie is an experienced data scientist specialized in NLP.",
    "career_history": [
      {"company": "Alibaba", "title": "Data Scientist", "start_date": "2019-03", "end_date": "2024-01"}
    ],
    "qa_status": "pass",
    "qa_issues": []
  }
]

4Ô∏è‚É£ Run the dashboard

In Replit shell:

streamlit run qa_dashboard.py


Then open the web view.
You‚Äôll see:

A filterable list of candidate profiles

Full biographies and structured histories

AI-flagged issues

Buttons to approve, reject, or edit and approve

A ‚ÄúSave QA Updates‚Äù button to persist your decisions

üß† Why This Matters

You now have a traceable, auditable QA process.

Both AI and human reviewers have clearly defined roles.

It‚Äôs modular ‚Äî later, you can:

Plug in a real database instead of JSON

Add login/auth for reviewers

Auto-push approved profiles into production

If you‚Äôd like, I can show you next how to integrate this dashboard with your staging and production databases ‚Äî so ‚ÄúApprove‚Äù literally moves a candidate from staging ‚Üí production tables automatically.

Would you like me to extend it that way?

You said:
Yes please 
ChatGPT said:

Perfect. üëå
Now that you have the QA Dashboard concept, let‚Äôs make it real by connecting it directly to your staging and production databases ‚Äî so the moment you click ‚ÄúApprove‚Äù, that candidate moves from staging ‚Üí production automatically.

Below is a design you can use right inside Replit with SQLite, which works out-of-the-box and later can be swapped for PostgreSQL or MySQL when you scale.

üß© ARCHITECTURE OVERVIEW
Raw Scraped Data ‚Üí Staging DB (awaiting verification)
       ‚Üì
AI Verification Pipeline
       ‚Üì
QA Dashboard (AI + Human review)
       ‚Üì
Production DB (verified, ready for matching)


We‚Äôll use two tables:

staging_candidates ‚Üí holds all unverified or in-review data

production_candidates ‚Üí holds AI- and human-approved records

üóÇ Step 1 ‚Äî Set up Database Tables

Create a file called database_setup.py (run it once):

import sqlite3

conn = sqlite3.connect("ai_recruiter.db")
cursor = conn.cursor()

# Table for staging (unverified / in-review candidates)
cursor.execute("""
CREATE TABLE IF NOT EXISTS staging_candidates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT,
    biography TEXT,
    career_history TEXT,
    qa_status TEXT,
    qa_issues TEXT,
    ai_proofread TEXT
)
""")

# Table for production (verified and ready)
cursor.execute("""
CREATE TABLE IF NOT EXISTS production_candidates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT,
    biography TEXT,
    career_history TEXT
)
""")

conn.commit()
conn.close()
print("‚úÖ Database and tables created successfully.")


Run it once in Replit:

python database_setup.py

üß† Step 2 ‚Äî Seed Some Sample Staging Data

Create seed_data.py (for testing):

import sqlite3, json

sample_candidates = [
    {
        "name": "Alex Chan",
        "biography": "Alex has 15 years of experience in HR and Finance.",
        "career_history": json.dumps([
            {"company": "HSBC", "title": "HR Director", "start_date": "2015-06", "end_date": "2023-08"},
            {"company": "PwC", "title": "HR Manager", "start_date": "2010-01", "end_date": "2015-05"}
        ]),
        "qa_status": "needs_review",
        "qa_issues": "Biography too short",
        "ai_proofread": "Alex has over 15 years of progressive experience in HR and Finance leadership."
    },
    {
        "name": "Jamie Li",
        "biography": "Jamie is an experienced data scientist in NLP.",
        "career_history": json.dumps([
            {"company": "Alibaba", "title": "Data Scientist", "start_date": "2019-03", "end_date": "2024-01"}
        ]),
        "qa_status": "pass",
        "qa_issues": "",
        "ai_proofread": ""
    }
]

conn = sqlite3.connect("ai_recruiter.db")
cur = conn.cursor()

for c in sample_candidates:
    cur.execute("""
    INSERT INTO staging_candidates (name, biography, career_history, qa_status, qa_issues, ai_proofread)
    VALUES (?, ?, ?, ?, ?, ?)
    """, (c["name"], c["biography"], c["career_history"], c["qa_status"], c["qa_issues"], c["ai_proofread"]))

conn.commit()
conn.close()
print("‚úÖ Sample candidates added to staging database.")


Run it:

python seed_data.py

üß∞ Step 3 ‚Äî Update the QA Dashboard to Work with the Database

Replace your previous qa_dashboard.py with this version:

import streamlit as st
import sqlite3, json, pandas as pd

# ===== Database Helpers =====
def get_staging_candidates(status_filter=None):
    conn = sqlite3.connect("ai_recruiter.db")
    query = "SELECT * FROM staging_candidates"
    if status_filter and status_filter != "All":
        query += f" WHERE qa_status='{status_filter.lower()}'"
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

def move_to_production(candidate_id):
    conn = sqlite3.connect("ai_recruiter.db")
    cur = conn.cursor()
    cur.execute("SELECT * FROM staging_candidates WHERE id=?", (candidate_id,))
    row = cur.fetchone()
    if row:
        cur.execute("""
        INSERT INTO production_candidates (name, biography, career_history)
        VALUES (?, ?, ?)
        """, (row[1], row[2], row[3]))
        cur.execute("DELETE FROM staging_candidates WHERE id=?", (candidate_id,))
        conn.commit()
    conn.close()

def update_candidate_status(candidate_id, status, new_bio=None):
    conn = sqlite3.connect("ai_recruiter.db")
    cur = conn.cursor()
    if new_bio:
        cur.execute("UPDATE staging_candidates SET qa_status=?, biography=? WHERE id=?", (status, new_bio, candidate_id))
    else:
        cur.execute("UPDATE staging_candidates SET qa_status=? WHERE id=?", (status, candidate_id))
    conn.commit()
    conn.close()

# ===== Streamlit UI =====
st.set_page_config(page_title="AI Recruiter QA Dashboard", layout="wide")
st.title("AI Recruiter ‚Äî QA Review Dashboard")

filter_option = st.sidebar.selectbox("Filter by status", ["All", "Needs Review", "Pass"])
df = get_staging_candidates(filter_option)

if len(df) == 0:
    st.info("‚úÖ No candidates to review.")
else:
    for _, row in df.iterrows():
        st.markdown("---")
        st.subheader(f"Candidate: {row['name']}")
        st.write(f"**Biography:** {row['biography']}")
        st.json(json.loads(row["career_history"]))
        st.write(f"**AI Issues:** {row['qa_issues']}")
        st.write(f"**AI Proofread Suggestion:** {row['ai_proofread'] or '‚Äî'}")

        action = st.radio(
            f"Action for {row['name']}:",
            ["Pending", "Approve", "Reject", "Edit & Approve"],
            key=f"action_{row['id']}",
            horizontal=True,
        )

        if action == "Approve":
            move_to_production(row["id"])
            st.success(f"‚úÖ {row['name']} approved and moved to production.")
        elif action == "Reject":
            update_candidate_status(row["id"], "rejected")
            st.error(f"‚ùå {row['name']} rejected for recheck.")
        elif action == "Edit & Approve":
            edited_bio = st.text_area("Edit biography", row["biography"], key=f"editbio_{row['id']}")
            update_candidate_status(row["id"], "pass", new_bio=edited_bio)
            move_to_production(row["id"])
            st.success(f"‚úèÔ∏è {row['name']} edited and approved.")



Run it:

streamlit run qa_dashboard.py

üß≠ What Happens Now

All pending candidates load directly from your staging database.

When you click Approve, the record automatically:

Moves to production_candidates

Is deleted from staging_candidates

‚ÄúReject‚Äù keeps it in staging but changes the qa_status to ‚Äúrejected‚Äù.

‚ÄúEdit & Approve‚Äù allows you to fix text before promoting.

üß† Future Add-Ons (When You‚Äôre Ready)

AI Proofread button (real-time GPT cleanup before approval)

Reviewer identity tracking (for audit trail)

Comments column (why approved/rejected)

Automatic logging to Google Sheets or Airtable for investor reporting

Email or Slack notifications for new candidates needing QA

Would you like me to add the AI Proofread button next ‚Äî so reviewers can click a single button to automatically improve the biography before approving?

You said:
yes please
ChatGPT said:

Fantastic üëå ‚Äî the AI Proofread button will make your QA Dashboard much smarter and more efficient.

Instead of rewriting bios manually, your reviewers can simply click ‚ÄúAI Proofread‚Äù, and the system will automatically improve the language, grammar, and tone (without changing factual content).

Here‚Äôs how to integrate it into your current qa_dashboard.py ‚Äî step by step.

üß† Step 1: Add OpenAI Dependency

In your Replit shell:

pip install openai


Then set your API key (in Replit Secrets ‚Üí OPENAI_API_KEY).

üß© Step 2: Update qa_dashboard.py with AI Proofreading

Replace the previous version with this one ‚Äî or update the new parts marked with üÜï comments.

import streamlit as st
import sqlite3, json, pandas as pd
import os
from openai import OpenAI

# üÜï Initialize OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ===== Database Helpers =====
def get_staging_candidates(status_filter=None):
    conn = sqlite3.connect("ai_recruiter.db")
    query = "SELECT * FROM staging_candidates"
    if status_filter and status_filter != "All":
        query += f" WHERE qa_status='{status_filter.lower()}'"
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

def move_to_production(candidate_id):
    conn = sqlite3.connect("ai_recruiter.db")
    cur = conn.cursor()
    cur.execute("SELECT * FROM staging_candidates WHERE id=?", (candidate_id,))
    row = cur.fetchone()
    if row:
        cur.execute("""
        INSERT INTO production_candidates (name, biography, career_history)
        VALUES (?, ?, ?)
        """, (row[1], row[2], row[3]))
        cur.execute("DELETE FROM staging_candidates WHERE id=?", (candidate_id,))
        conn.commit()
    conn.close()

def update_candidate_status(candidate_id, status, new_bio=None, ai_proofread=None):
    conn = sqlite3.connect("ai_recruiter.db")
    cur = conn.cursor()
    if new_bio and ai_proofread:
        cur.execute("UPDATE staging_candidates SET qa_status=?, biography=?, ai_proofread=? WHERE id=?",
                    (status, new_bio, ai_proofread, candidate_id))
    elif new_bio:
        cur.execute("UPDATE staging_candidates SET qa_status=?, biography=? WHERE id=?",
                    (status, new_bio, candidate_id))
    elif ai_proofread:
        cur.execute("UPDATE staging_candidates SET ai_proofread=? WHERE id=?",
                    (ai_proofread, candidate_id))
    else:
        cur.execute("UPDATE staging_candidates SET qa_status=? WHERE id=?", (status, candidate_id))
    conn.commit()
    conn.close()

# üÜï AI Proofreading Function
def ai_proofread_biography(bio_text: str) -> str:
    """Call GPT to refine the biography text."""
    prompt = f"""
    You are an executive recruiter assistant.
    Proofread and refine this biography for grammar, clarity, and tone.
    Keep all factual details. Make it concise and professional:
    {bio_text}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a professional HR editor."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        st.error(f"AI Proofreading failed: {e}")
        return bio_text

# ===== Streamlit UI =====
st.set_page_config(page_title="AI Recruiter QA Dashboard", layout="wide")
st.title("AI Recruiter ‚Äî QA Review Dashboard")

filter_option = st.sidebar.selectbox("Filter by status", ["All", "Needs Review", "Pass"])
df = get_staging_candidates(filter_option)

if len(df) == 0:
    st.info("‚úÖ No candidates to review.")
else:
    for _, row in df.iterrows():
        st.markdown("---")
        st.subheader(f"Candidate: {row['name']}")
        st.write(f"**Original Biography:** {row['biography']}")
        st.json(json.loads(row["career_history"]))
        st.write(f"**AI Issues:** {row['qa_issues'] or '‚Äî'}")

        if row["ai_proofread"]:
            st.write(f"**AI Proofread Version:** {row['ai_proofread']}")
        else:
            # üÜï Add AI Proofread button
            if st.button(f"‚ú® AI Proofread {row['name']}", key=f"proofread_{row['id']}"):
                improved_bio = ai_proofread_biography(row["biography"])
                update_candidate_status(row["id"], row["qa_status"], ai_proofread=improved_bio)
                st.success("AI-proofread version saved. Refresh to view.")

        action = st.radio(
            f"Action for {row['name']}:",
            ["Pending", "Approve", "Reject", "Edit & Approve"],
            key=f"action_{row['id']}",
            horizontal=True,
        )

        if action == "Approve":
            move_to_production(row["id"])
            st.success(f"‚úÖ {row['name']} approved and moved to production.")
        elif action == "Reject":
            update_candidate_status(row["id"], "rejected")
            st.error(f"‚ùå {row['name']} rejected for recheck.")
        elif action == "Edit & Approve":
            edited_bio = st.text_area("Edit biography", row["ai_proofread"] or row["biography"], key=f"editbio_{row['id']}")
            update_candidate_status(row["id"], "pass", new_bio=edited_bio)
            move_to_production(row["id"])
            st.success(f"‚úèÔ∏è {row['name']} edited and approved.")

üß≠ Step 3: How It Works Now

Once you restart Streamlit (streamlit run qa_dashboard.py):

Each candidate shows:

Original biography

Career history

AI-flagged issues

If no AI-proofread version exists ‚Üí you‚Äôll see a ‚Äú‚ú® AI Proofread‚Äù button.

Click it ‚Üí calls GPT ‚Üí rewrites the bio ‚Üí saves it to the database.

You can then:

Compare the AI version

Approve / Reject / Edit before pushing to production

‚úÖ Benefits

Human reviewers work faster ‚Äî less manual rewriting.

AI improves professionalism and tone consistently.

Full traceability ‚Äî original and AI-proofread versions both stored.

No factual drift ‚Äî prompt ensures details remain unchanged.

Would you like me to show you next how to add a side-by-side comparison view (original vs AI-proofread biography) with an ‚ÄúAccept AI Version‚Äù button ‚Äî so your QA reviewer can easily decide which version to keep?

You said:
yes please
ChatGPT said:

Perfect üëç ‚Äî let‚Äôs make your QA dashboard even smarter by giving reviewers a side-by-side view of the biography before and after AI proofreading ‚Äî with one click to ‚ÄúAccept AI Version.‚Äù

This is how professional-grade AI editing tools (like Grammarly or Copy.ai) handle human QA ‚Äî and it fits beautifully into your ‚ÄúAI + human recruiter‚Äù workflow.

üß± Concept Overview

You‚Äôll add:

üß© A split layout: Original Bio on the left, AI-Proofread Bio on the right.

‚úÖ Buttons to Accept AI version, Edit manually, or Reject.

üß† When a reviewer accepts the AI version ‚Üí it replaces the original in the database and moves to production.

üß© Step 1: Update the QA Dashboard

Below is the improved version of your qa_dashboard.py.
I‚Äôll highlight new parts with comments # üÜï.

import streamlit as st
import sqlite3, json, pandas as pd
import os
from openai import OpenAI

# üß† Initialize OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ===== Database Helpers =====
def get_staging_candidates(status_filter=None):
    conn = sqlite3.connect("ai_recruiter.db")
    query = "SELECT * FROM staging_candidates"
    if status_filter and status_filter != "All":
        query += f" WHERE qa_status='{status_filter.lower()}'"
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

def move_to_production(candidate_id):
    conn = sqlite3.connect("ai_recruiter.db")
    cur = conn.cursor()
    cur.execute("SELECT * FROM staging_candidates WHERE id=?", (candidate_id,))
    row = cur.fetchone()
    if row:
        cur.execute("""
        INSERT INTO production_candidates (name, biography, career_history)
        VALUES (?, ?, ?)
        """, (row[1], row[2], row[3]))
        cur.execute("DELETE FROM staging_candidates WHERE id=?", (candidate_id,))
        conn.commit()
    conn.close()

def update_candidate_status(candidate_id, status, new_bio=None, ai_proofread=None):
    conn = sqlite3.connect("ai_recruiter.db")
    cur = conn.cursor()
    if new_bio and ai_proofread:
        cur.execute("UPDATE staging_candidates SET qa_status=?, biography=?, ai_proofread=? WHERE id=?",
                    (status, new_bio, ai_proofread, candidate_id))
    elif new_bio:
        cur.execute("UPDATE staging_candidates SET qa_status=?, biography=? WHERE id=?",
                    (status, new_bio, candidate_id))
    elif ai_proofread:
        cur.execute("UPDATE staging_candidates SET ai_proofread=? WHERE id=?",
                    (ai_proofread, candidate_id))
    else:
        cur.execute("UPDATE staging_candidates SET qa_status=? WHERE id=?", (status, candidate_id))
    conn.commit()
    conn.close()

# ===== AI Proofreading =====
def ai_proofread_biography(bio_text: str) -> str:
    prompt = f"""
    You are an executive recruiter assistant.
    Proofread and refine this biography for grammar, clarity, and tone.
    Keep all factual details intact. Make it concise and professional.
    ---
    {bio_text}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a professional HR editor."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        st.error(f"AI Proofreading failed: {e}")
        return bio_text

# ===== Streamlit UI =====
st.set_page_config(page_title="AI Recruiter QA Dashboard", layout="wide")
st.title("AI Recruiter ‚Äî QA Review Dashboard")

filter_option = st.sidebar.selectbox("Filter by status", ["All", "Needs Review", "Pass"])
df = get_staging_candidates(filter_option)

if len(df) == 0:
    st.info("‚úÖ No candidates to review.")
else:
    for _, row in df.iterrows():
        st.markdown("---")
        st.subheader(f"üë§ Candidate: {row['name']}")
        st.json(json.loads(row["career_history"]))

        # üß© Display two columns for comparison
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### üìù Original Biography")
            st.write(row["biography"])

        with col2:
            st.markdown("### ü§ñ AI-Proofread Version")

            if not row["ai_proofread"]:
                if st.button(f"‚ú® Generate AI Proofread for {row['name']}", key=f"proof_{row['id']}"):
                    improved_bio = ai_proofread_biography(row["biography"])
                    update_candidate_status(row["id"], row["qa_status"], ai_proofread=improved_bio)
                    st.success("AI version created. Refresh to view.")
            else:
                st.write(row["ai_proofread"])

        # üÜï Add action buttons below comparison
        st.markdown("#### Review Actions")

        colA, colB, colC = st.columns([1, 1, 1])
        with colA:
            if st.button(f"‚úÖ Accept AI Version ‚Äî {row['name']}", key=f"accept_{row['id']}"):
                update_candidate_status(row["id"], "pass", new_bio=row["ai_proofread"])
                move_to_production(row["id"])
                st.success(f"‚úÖ {row['name']} approved with AI version.")

        with colB:
            if st.button(f"‚úèÔ∏è Edit & Approve ‚Äî {row['name']}", key=f"edit_{row['id']}"):
                new_text = st.text_area("Edit biography:", row["ai_proofread"] or row["biography"])
                if st.button(f"Save Edits for {row['name']}", key=f"saveedit_{row['id']}"):
                    update_candidate_status(row["id"], "pass", new_bio=new_text)
                    move_to_production(row["id"])
                    st.success(f"üìù {row['name']} edited and approved.")

        with colC:
            if st.button(f"‚ùå Reject ‚Äî {row['name']}", key=f"reject_{row['id']}"):
                update_candidate_status(row["id"], "rejected")
                st.error(f"‚ùå {row['name']} rejected for recheck.")

üß≠ Step 2: How Reviewers Use It

Original Bio (left) ‚Üí unedited text from AI sourcing.

AI-Proofread Bio (right) ‚Üí cleaned version (once generated).

Action Buttons:

‚ú® Generate AI version (if missing)

‚úÖ Accept AI Version (auto-approve + move to production)

‚úèÔ∏è Edit & Approve (open editable text area)

‚ùå Reject (flag for further review)

üß† Optional Upgrade: Confidence Scoring

Later, we can add an AI QA Scoring System, e.g.:

Metric	Example Logic
Grammar	GPT checks clarity & conciseness
Tone	Consistency with professional style
Completeness	Detects missing company/role details
Factual Risk	Flags if GPT possibly changed a detail

‚Üí Then show a color-coded bar (üü¢üü°üî¥) for each metric to help human QA focus attention.

Would you like me to show you how to build the AI QA scoring system next ‚Äî so the dashboard automatically rates each bio‚Äôs quality and flags potential factual risks before human review?

You said:
Yes please 
ChatGPT said:

Excellent ‚Äî this next layer turns your QA Dashboard into a semi-autonomous reviewer, capable of judging each biography‚Äôs quality before a human even looks at it.

This ‚ÄúAI QA Scoring System‚Äù will give every candidate biography a numeric and color-coded score across four key metrics:

Metric	Description	Example Range
Clarity	Grammar, flow, sentence structure	0‚Äì10
Professional Tone	Fit with executive / HR style	0‚Äì10
Completeness	Career timeline consistency, missing data	0‚Äì10
Factual Consistency	Detects if details were altered during proofreading	0‚Äì10

The overall QA Score = weighted average of these four.
If the total is below a threshold (e.g., 7/10), it‚Äôs automatically flagged as Needs Human Review.

üß† Step 1: Extend the Database

In your SQLite setup, add two new fields to staging_candidates:

ALTER TABLE staging_candidates
ADD COLUMN qa_score JSON;


This will store the structured scores per metric as JSON.

üß© Step 2: Add the Scoring Function to Your qa_dashboard.py

Insert this helper just below the proofreading function:

# ===== AI QA Scoring =====
def ai_evaluate_biography(original_bio: str, ai_bio: str) -> dict:
    """
    Returns a JSON object like:
    {
      "clarity": 8.5,
      "tone": 9,
      "completeness": 7,
      "factual_consistency": 8,
      "comments": "Strong flow, but missing one company role detail."
    }
    """
    prompt = f"""
    You are an expert recruiter trained in HR communication standards.
    Evaluate the following two biographies:
    1. Original version:
    {original_bio}

    2. AI-proofread version:
    {ai_bio}

    For each metric, give a score from 0‚Äì10 and a brief comment:
    - Clarity: grammar, structure, readability
    - Professional Tone: suitability for executive audience
    - Completeness: whether it covers the key career information
    - Factual Consistency: ensure facts were not changed or invented

    Return results in JSON like this:
    {{
      "clarity": 0-10,
      "tone": 0-10,
      "completeness": 0-10,
      "factual_consistency": 0-10,
      "comments": "summary..."
    }}
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an HR QA evaluator."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        import json
        result = response.choices[0].message.content.strip()
        return json.loads(result)
    except Exception as e:
        st.error(f"QA scoring failed: {e}")
        return {
            "clarity": 0, "tone": 0, "completeness": 0,
            "factual_consistency": 0, "comments": "Error"
        }

üß© Step 3: Add a ‚ÄúRun QA Evaluation‚Äù Button in the Dashboard

In the candidate display section, right under the ‚ÄúAI Proofread‚Äù column, add:

if row["ai_proofread"]:
    if st.button(f"üîç Run QA Evaluation for {row['name']}", key=f"eval_{row['id']}"):
        qa_scores = ai_evaluate_biography(row["biography"], row["ai_proofread"])
        conn = sqlite3.connect("ai_recruiter.db")
        cur = conn.cursor()
        cur.execute("UPDATE staging_candidates SET qa_score=? WHERE id=?", 
                    (json.dumps(qa_scores), row["id"]))
        conn.commit()
        conn.close()
        st.success("‚úÖ QA evaluation completed. Refresh to see scores.")

üß© Step 4: Display the Scores Visually

Still in the same candidate section, after the ‚ÄúReview Actions‚Äù, add:

if row.get("qa_score"):
    try:
        score_data = json.loads(row["qa_score"])
        overall = round(
            (score_data["clarity"] + score_data["tone"] + 
             score_data["completeness"] + score_data["factual_consistency"]) / 4, 2
        )
        color = "üü¢" if overall >= 8 else "üü°" if overall >= 6 else "üî¥"
        st.markdown(f"### QA Score: {color} {overall}/10")
        st.write(score_data)
    except Exception:
        pass


Now each candidate will show a summary like:

QA Score: üü° 7.25/10
{"clarity": 8.5, "tone": 9, "completeness": 6, "factual_consistency": 6.5,
 "comments": "Good tone but missing a job transition date."}

üßÆ Step 5: Auto-Flag Low Scores

You can add a simple rule: if overall score < 7, automatically mark qa_status = "needs_review".

Insert this logic right after saving the scores:

if overall < 7:
    update_candidate_status(row["id"], "needs_review")
    st.warning(f"‚ö†Ô∏è {row['name']} flagged for human QA review (score below threshold).")

üß≠ What You Now Have

‚úÖ AI Proofreading ‚Üí AI improves clarity and tone
‚úÖ AI QA Scoring ‚Üí AI rates each biography‚Äôs quality
‚úÖ Human QA Review ‚Üí Only needed for flagged profiles
‚úÖ Production Transfer ‚Üí Approved profiles move seamlessly

This gives you a system that behaves like:

‚ÄúA recruiter with 15 years‚Äô experience reviewing 100 bios at once ‚Äî but never gets tired.‚Äù

Would you like me to show you next how to make the AI QA Scoring automatic (so it runs for every new candidate added to the staging database without human clicking)?